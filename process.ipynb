{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T09:12:33.634461Z",
     "start_time": "2025-04-04T09:12:33.155704Z"
    }
   },
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Volume Trend Line Chart (Dashboard 1)",
   "id": "494ac794e9152466"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T09:19:07.708751Z",
     "start_time": "2025-04-04T09:18:58.377163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_volume_trend(view=\"24h\"):\n",
    "    # Load CSVs\n",
    "    solver_df = pd.read_csv(\"data/solver_competitions.csv\")\n",
    "    tx_df = pd.read_csv(\"data/transaction.csv\")\n",
    "\n",
    "    # Merge on auctionId\n",
    "    merged = pd.merge(solver_df, tx_df, on=\"auctionId\", how=\"inner\")\n",
    "\n",
    "    # Compute volume\n",
    "    merged[\"volume\"] = merged[[\"buyAmountInUSD\", \"sellAmountInUSD\"]].min(axis=1)\n",
    "    merged[\"timestamp\"] = pd.to_datetime(merged[\"timestamp\"])\n",
    "\n",
    "    # Filter by time range\n",
    "    now = merged[\"timestamp\"].max()  # Use latest timestamp in data\n",
    "    if view == \"24h\":\n",
    "        cutoff = now - timedelta(hours=24)\n",
    "        freq = \"1h\"\n",
    "    elif view == \"7d\":\n",
    "        cutoff = now - timedelta(days=7)\n",
    "        freq = \"4h\"\n",
    "    else:\n",
    "        cutoff = merged[\"timestamp\"].min()\n",
    "        freq = \"1H\"  # default bin size\n",
    "\n",
    "    merged = merged[merged[\"timestamp\"] >= cutoff]\n",
    "\n",
    "    # Bin by time\n",
    "    merged[\"time_bin\"] = merged[\"timestamp\"].dt.floor(freq)\n",
    "\n",
    "    # Aggregate volume\n",
    "    trend = merged.groupby(\"time_bin\")[\"volume\"].sum().reset_index()\n",
    "    trend[\"time_bin\"] = trend[\"time_bin\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "    trend.rename(columns={\"time_bin\": \"timestamp\"}, inplace=True)\n",
    "\n",
    "    # Save to JSON\n",
    "    output_file = f\"out/volume_trend_{view}.json\"\n",
    "    trend.to_json(output_file, orient=\"records\")\n",
    "    print(f\"Wrote {output_file}\")\n",
    "\n",
    "generate_volume_trend(\"24h\")\n",
    "generate_volume_trend(\"7d\")"
   ],
   "id": "bd8ecc409215d0a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote out/volume_trend_24h.json\n",
      "Wrote out/volume_trend_7d.json\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Order Solver Time Difference (Dashboard 1)",
   "id": "53393dedeaf1daaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T10:38:48.535435Z",
     "start_time": "2025-04-04T10:38:34.349769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_order_solver_time_diff(\n",
    "    solver_path=\"data/solver_competitions.csv\",\n",
    "    tx_path=\"data/transaction.csv\",\n",
    "    order_path=\"data/order.csv\",\n",
    "    output_path=\"out/order_solver_time_diff.json\"\n",
    "):\n",
    "    # Load data\n",
    "    solver_df = pd.read_csv(solver_path)\n",
    "    tx_df = pd.read_csv(tx_path)\n",
    "    order_df = pd.read_csv(order_path)\n",
    "\n",
    "    # ðŸ›  Fix: clean timestamps before merge\n",
    "    tx_df[\"timestamp\"] = pd.to_datetime(tx_df[\"timestamp\"], utc=True).dt.tz_localize(None)\n",
    "    order_df[\"createdTimestamp\"] = pd.to_datetime(order_df[\"createdTimestamp\"], utc=True).dt.tz_localize(None)\n",
    "\n",
    "    # Merge\n",
    "    merged = pd.merge(solver_df, tx_df, on=\"auctionId\", how=\"inner\")\n",
    "    merged = pd.merge(merged, order_df, left_on=\"orderId\", right_on=\"id\", how=\"inner\")\n",
    "\n",
    "    # Compute time difference in seconds\n",
    "    merged[\"time_diff_sec\"] = (merged[\"timestamp\"] - merged[\"createdTimestamp\"]).dt.total_seconds()\n",
    "    merged = merged.dropna(subset=[\"time_diff_sec\"])\n",
    "\n",
    "    # Filter between 0â€“180s\n",
    "    merged = merged[(merged[\"time_diff_sec\"] >= 0) & (merged[\"time_diff_sec\"] <= 180)]\n",
    "\n",
    "    # Bin into ranges (10s buckets)\n",
    "    bins = list(range(0, 190, 10))\n",
    "    labels = [f\"{i}-{i+10}\" for i in bins[:-1]]\n",
    "    merged[\"bucket\"] = pd.cut(merged[\"time_diff_sec\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Count per bucket\n",
    "    result = merged.groupby(\"bucket\", observed=True).size().reset_index(name=\"count\")\n",
    "    result = result.dropna()\n",
    "    result[\"bucket\"] = result[\"bucket\"].astype(str)\n",
    "\n",
    "    # Save output\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    result.to_json(output_path, orient=\"records\", indent=2)\n",
    "    print(f\"[âœ“] Wrote {len(result)} bins to {output_path}\")\n",
    "\n",
    "# Run\n",
    "generate_order_solver_time_diff()\n"
   ],
   "id": "c5c2e6c38225e6f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Wrote 14 bins to out/order_solver_time_diff.json\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Solver Participation % (Dashboard 1)",
   "id": "8e0784c6c21e64fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T11:01:57.396323Z",
     "start_time": "2025-04-04T11:01:54.026437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_solver_participation(\n",
    "    participation_path=\"data/solver_competitions.csv\",\n",
    "    solver_labels_path=\"data/solver.csv\",\n",
    "    output_path=\"out/solver_participation.json\"\n",
    "):\n",
    "    df = pd.read_csv(participation_path)\n",
    "    label_df = pd.read_csv(solver_labels_path)\n",
    "\n",
    "    # Drop missing\n",
    "    df = df.dropna(subset=[\"solverAddress\", \"auctionId\"])\n",
    "\n",
    "    # Total auctions\n",
    "    total_auctions = df[\"auctionId\"].nunique()\n",
    "\n",
    "    # Unique auctions per solver\n",
    "    participation = df.groupby(\"solverAddress\")[\"auctionId\"].nunique().reset_index(name=\"auctions\")\n",
    "    participation[\"participation_pct\"] = (participation[\"auctions\"] / total_auctions * 100).round(2)\n",
    "\n",
    "    # Merge with labelName\n",
    "    participation = pd.merge(participation, label_df, left_on=\"solverAddress\", right_on=\"address\", how=\"left\")\n",
    "    participation[\"name\"] = participation[\"labelName\"].fillna(participation[\"solverAddress\"])\n",
    "\n",
    "    # Sort and limit\n",
    "    participation = participation.sort_values(by=\"participation_pct\", ascending=False).head(20)\n",
    "\n",
    "    # Rename for frontend\n",
    "    participation = participation[[\"solverAddress\", \"name\", \"participation_pct\"]]\n",
    "    participation = participation.rename(columns={\"solverAddress\": \"solver\"})\n",
    "\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    participation.to_json(output_path, orient=\"records\", indent=2)\n",
    "    print(f\"[âœ“] Wrote {len(participation)} labeled solvers to {output_path}\")\n",
    "\n",
    "generate_solver_participation()"
   ],
   "id": "9f269ad02b906b01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Wrote 20 labeled solvers to out/solver_participation.json\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Token Pair Treemap",
   "id": "e3513130204765c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:22:19.317690Z",
     "start_time": "2025-04-05T10:21:43.755481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_token_pair_treemap_with_bins(\n",
    "    comp_path=\"data/solver_competitions.csv\",\n",
    "    order_path=\"data/order.csv\",\n",
    "    token_labels_path=\"data/token_labels.csv\",\n",
    "    output_path=\"out/treemap_token_pair_volume.json\"\n",
    "):\n",
    "    comp_df = pd.read_csv(comp_path)\n",
    "    order_df = pd.read_csv(order_path)\n",
    "    label_df = pd.read_csv(token_labels_path, on_bad_lines=\"skip\")\n",
    "\n",
    "    order_df = order_df.rename(columns={\n",
    "        \"buyAmountInUSD\": \"orderBuyUSD\",\n",
    "        \"sellAmountInUSD\": \"orderSellUSD\",\n",
    "    })\n",
    "\n",
    "    # Keep only relevant columns\n",
    "    label_df = label_df[[\"address\", \"symbol\"]]\n",
    "    merged = pd.merge(comp_df, order_df, left_on=\"orderId\", right_on=\"id\", how=\"inner\")\n",
    "    merged = pd.merge(merged, label_df, left_on=\"sellTokenAddress\", right_on=\"address\", how=\"left\").rename(columns={\"symbol\": \"sellSymbol\"})\n",
    "    merged = pd.merge(merged, label_df, left_on=\"buyTokenAddress\", right_on=\"address\", how=\"left\").rename(columns={\"symbol\": \"buySymbol\"})\n",
    "\n",
    "    merged = merged.dropna(subset=[\"sellSymbol\", \"buySymbol\", \"buyAmountInUSD\", \"sellAmountInUSD\"])\n",
    "    merged[\"tokenPair\"] = merged.apply(\n",
    "        lambda row: \"-\".join(sorted([row[\"sellSymbol\"], row[\"buySymbol\"]])),\n",
    "        axis=1\n",
    "    )\n",
    "    merged[\"volume\"] = merged[[\"buyAmountInUSD\", \"sellAmountInUSD\"]].min(axis=1)\n",
    "\n",
    "    # Bin volumes\n",
    "    bins = [0, 1000] + [1000 + i * 1000 for i in range(1, 11)] + [np.inf]\n",
    "    labels = [f\"{int(bins[i])}-{int(bins[i+1]) if bins[i+1] != np.inf else '10000+'}\" for i in range(len(bins) - 1)]\n",
    "    merged[\"volume_bin\"] = pd.cut(merged[\"volume\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Group by tokenPair\n",
    "    grouped = merged.groupby(\"tokenPair\")\n",
    "\n",
    "    output = []\n",
    "    for pair, group in grouped:\n",
    "        bin_counts = group[\"volume_bin\"].value_counts().reindex(labels, fill_value=0).to_dict()\n",
    "        total_volume = group[\"volume\"].sum()\n",
    "        output.append({\n",
    "            \"tokenPair\": pair,\n",
    "            \"value\": total_volume,\n",
    "            \"binCounts\": bin_counts\n",
    "        })\n",
    "\n",
    "    # Sort and keep top 100\n",
    "    output = sorted(output, key=lambda x: x[\"value\"], reverse=True)[:100]\n",
    "\n",
    "    # Save\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame(output).to_json(output_path, orient=\"records\", indent=2)\n",
    "    print(f\"[âœ“] Saved treemap data with bin counts to {output_path}\")\n",
    "\n",
    "generate_token_pair_treemap_with_bins()"
   ],
   "id": "bc9ca9dc8941cfa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Saved treemap data with bin counts to out/treemap_token_pair_volume.json\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
