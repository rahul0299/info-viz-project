{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Volume Trend Line Chart (Dashboard 1)",
   "id": "494ac794e9152466"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T09:19:07.708751Z",
     "start_time": "2025-04-04T09:18:58.377163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_volume_trend(view=\"24h\"):\n",
    "    # Load CSVs\n",
    "    solver_df = pd.read_csv(\"data/solver_competitions.csv\")\n",
    "    tx_df = pd.read_csv(\"data/transaction.csv\")\n",
    "\n",
    "    # Merge on auctionId\n",
    "    merged = pd.merge(solver_df, tx_df, on=\"auctionId\", how=\"inner\")\n",
    "\n",
    "    # Compute volume\n",
    "    merged[\"volume\"] = merged[[\"buyAmountInUSD\", \"sellAmountInUSD\"]].min(axis=1)\n",
    "    merged[\"timestamp\"] = pd.to_datetime(merged[\"timestamp\"])\n",
    "\n",
    "    # Filter by time range\n",
    "    now = merged[\"timestamp\"].max()  # Use latest timestamp in data\n",
    "    if view == \"24h\":\n",
    "        cutoff = now - timedelta(hours=24)\n",
    "        freq = \"1h\"\n",
    "    elif view == \"7d\":\n",
    "        cutoff = now - timedelta(days=7)\n",
    "        freq = \"4h\"\n",
    "    else:\n",
    "        cutoff = merged[\"timestamp\"].min()\n",
    "        freq = \"1H\"  # default bin size\n",
    "\n",
    "    merged = merged[merged[\"timestamp\"] >= cutoff]\n",
    "\n",
    "    # Bin by time\n",
    "    merged[\"time_bin\"] = merged[\"timestamp\"].dt.floor(freq)\n",
    "\n",
    "    # Aggregate volume\n",
    "    trend = merged.groupby(\"time_bin\")[\"volume\"].sum().reset_index()\n",
    "    trend[\"time_bin\"] = trend[\"time_bin\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "    trend.rename(columns={\"time_bin\": \"timestamp\"}, inplace=True)\n",
    "\n",
    "    # Save to JSON\n",
    "    output_file = f\"out/volume_trend_{view}.json\"\n",
    "    trend.to_json(output_file, orient=\"records\")\n",
    "    print(f\"Wrote {output_file}\")\n",
    "\n",
    "generate_volume_trend(\"24h\")\n",
    "generate_volume_trend(\"7d\")"
   ],
   "id": "bd8ecc409215d0a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote out/volume_trend_24h.json\n",
      "Wrote out/volume_trend_7d.json\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Order Solver Time Difference (Dashboard 1)",
   "id": "53393dedeaf1daaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T10:38:48.535435Z",
     "start_time": "2025-04-04T10:38:34.349769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_order_solver_time_diff(\n",
    "    solver_path=\"data/solver_competitions.csv\",\n",
    "    tx_path=\"data/transaction.csv\",\n",
    "    order_path=\"data/order.csv\",\n",
    "    output_path=\"out/order_solver_time_diff.json\"\n",
    "):\n",
    "    # Load data\n",
    "    solver_df = pd.read_csv(solver_path)\n",
    "    tx_df = pd.read_csv(tx_path)\n",
    "    order_df = pd.read_csv(order_path)\n",
    "\n",
    "    # ðŸ›  Fix: clean timestamps before merge\n",
    "    tx_df[\"timestamp\"] = pd.to_datetime(tx_df[\"timestamp\"], utc=True).dt.tz_localize(None)\n",
    "    order_df[\"createdTimestamp\"] = pd.to_datetime(order_df[\"createdTimestamp\"], utc=True).dt.tz_localize(None)\n",
    "\n",
    "    # Merge\n",
    "    merged = pd.merge(solver_df, tx_df, on=\"auctionId\", how=\"inner\")\n",
    "    merged = pd.merge(merged, order_df, left_on=\"orderId\", right_on=\"id\", how=\"inner\")\n",
    "\n",
    "    # Compute time difference in seconds\n",
    "    merged[\"time_diff_sec\"] = (merged[\"timestamp\"] - merged[\"createdTimestamp\"]).dt.total_seconds()\n",
    "    merged = merged.dropna(subset=[\"time_diff_sec\"])\n",
    "\n",
    "    # Filter between 0â€“180s\n",
    "    merged = merged[(merged[\"time_diff_sec\"] >= 0) & (merged[\"time_diff_sec\"] <= 180)]\n",
    "\n",
    "    # Bin into ranges (10s buckets)\n",
    "    bins = list(range(0, 190, 10))\n",
    "    labels = [f\"{i}-{i+10}\" for i in bins[:-1]]\n",
    "    merged[\"bucket\"] = pd.cut(merged[\"time_diff_sec\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Count per bucket\n",
    "    result = merged.groupby(\"bucket\", observed=True).size().reset_index(name=\"count\")\n",
    "    result = result.dropna()\n",
    "    result[\"bucket\"] = result[\"bucket\"].astype(str)\n",
    "\n",
    "    # Save output\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    result.to_json(output_path, orient=\"records\", indent=2)\n",
    "    print(f\"[âœ“] Wrote {len(result)} bins to {output_path}\")\n",
    "\n",
    "# Run\n",
    "generate_order_solver_time_diff()\n"
   ],
   "id": "c5c2e6c38225e6f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Wrote 14 bins to out/order_solver_time_diff.json\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Solver Participation % (Dashboard 1)",
   "id": "8e0784c6c21e64fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T11:01:57.396323Z",
     "start_time": "2025-04-04T11:01:54.026437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_solver_participation(\n",
    "    participation_path=\"data/solver_competitions.csv\",\n",
    "    solver_labels_path=\"data/solver.csv\",\n",
    "    output_path=\"out/solver_participation.json\"\n",
    "):\n",
    "    df = pd.read_csv(participation_path)\n",
    "    label_df = pd.read_csv(solver_labels_path)\n",
    "\n",
    "    # Drop missing\n",
    "    df = df.dropna(subset=[\"solverAddress\", \"auctionId\"])\n",
    "\n",
    "    # Total auctions\n",
    "    total_auctions = df[\"auctionId\"].nunique()\n",
    "\n",
    "    # Unique auctions per solver\n",
    "    participation = df.groupby(\"solverAddress\")[\"auctionId\"].nunique().reset_index(name=\"auctions\")\n",
    "    participation[\"participation_pct\"] = (participation[\"auctions\"] / total_auctions * 100).round(2)\n",
    "\n",
    "    # Merge with labelName\n",
    "    participation = pd.merge(participation, label_df, left_on=\"solverAddress\", right_on=\"address\", how=\"left\")\n",
    "    participation[\"name\"] = participation[\"labelName\"].fillna(participation[\"solverAddress\"])\n",
    "\n",
    "    # Sort and limit\n",
    "    participation = participation.sort_values(by=\"participation_pct\", ascending=False).head(20)\n",
    "\n",
    "    # Rename for frontend\n",
    "    participation = participation[[\"solverAddress\", \"name\", \"participation_pct\"]]\n",
    "    participation = participation.rename(columns={\"solverAddress\": \"solver\"})\n",
    "\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    participation.to_json(output_path, orient=\"records\", indent=2)\n",
    "    print(f\"[âœ“] Wrote {len(participation)} labeled solvers to {output_path}\")\n",
    "\n",
    "generate_solver_participation()"
   ],
   "id": "9f269ad02b906b01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Wrote 20 labeled solvers to out/solver_participation.json\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Token Pair Treemap",
   "id": "e3513130204765c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T10:22:19.317690Z",
     "start_time": "2025-04-05T10:21:43.755481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_token_pair_treemap_with_bins(\n",
    "    comp_path=\"data/solver_competitions.csv\",\n",
    "    order_path=\"data/order.csv\",\n",
    "    token_labels_path=\"data/token_labels.csv\",\n",
    "    output_path=\"out/treemap_token_pair_volume.json\"\n",
    "):\n",
    "    comp_df = pd.read_csv(comp_path)\n",
    "    order_df = pd.read_csv(order_path)\n",
    "    label_df = pd.read_csv(token_labels_path, on_bad_lines=\"skip\")\n",
    "\n",
    "    order_df = order_df.rename(columns={\n",
    "        \"buyAmountInUSD\": \"orderBuyUSD\",\n",
    "        \"sellAmountInUSD\": \"orderSellUSD\",\n",
    "    })\n",
    "\n",
    "    # Keep only relevant columns\n",
    "    label_df = label_df[[\"address\", \"symbol\"]]\n",
    "    merged = pd.merge(comp_df, order_df, left_on=\"orderId\", right_on=\"id\", how=\"inner\")\n",
    "    merged = pd.merge(merged, label_df, left_on=\"sellTokenAddress\", right_on=\"address\", how=\"left\").rename(columns={\"symbol\": \"sellSymbol\"})\n",
    "    merged = pd.merge(merged, label_df, left_on=\"buyTokenAddress\", right_on=\"address\", how=\"left\").rename(columns={\"symbol\": \"buySymbol\"})\n",
    "\n",
    "    merged = merged.dropna(subset=[\"sellSymbol\", \"buySymbol\", \"buyAmountInUSD\", \"sellAmountInUSD\"])\n",
    "    merged[\"tokenPair\"] = merged.apply(\n",
    "        lambda row: \"-\".join(sorted([row[\"sellSymbol\"], row[\"buySymbol\"]])),\n",
    "        axis=1\n",
    "    )\n",
    "    merged[\"volume\"] = merged[[\"buyAmountInUSD\", \"sellAmountInUSD\"]].min(axis=1)\n",
    "\n",
    "    # Bin volumes\n",
    "    bins = [0, 1000] + [1000 + i * 1000 for i in range(1, 11)] + [np.inf]\n",
    "    labels = [f\"{int(bins[i])}-{int(bins[i+1]) if bins[i+1] != np.inf else '10000+'}\" for i in range(len(bins) - 1)]\n",
    "    merged[\"volume_bin\"] = pd.cut(merged[\"volume\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Group by tokenPair\n",
    "    grouped = merged.groupby(\"tokenPair\")\n",
    "\n",
    "    output = []\n",
    "    for pair, group in grouped:\n",
    "        bin_counts = group[\"volume_bin\"].value_counts().reindex(labels, fill_value=0).to_dict()\n",
    "        total_volume = group[\"volume\"].sum()\n",
    "        output.append({\n",
    "            \"tokenPair\": pair,\n",
    "            \"value\": total_volume,\n",
    "            \"binCounts\": bin_counts\n",
    "        })\n",
    "\n",
    "    # Sort and keep top 100\n",
    "    output = sorted(output, key=lambda x: x[\"value\"], reverse=True)[:100]\n",
    "\n",
    "    # Save\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame(output).to_json(output_path, orient=\"records\", indent=2)\n",
    "    print(f\"[âœ“] Saved treemap data with bin counts to {output_path}\")\n",
    "\n",
    "generate_token_pair_treemap_with_bins()"
   ],
   "id": "bc9ca9dc8941cfa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Saved treemap data with bin counts to out/treemap_token_pair_volume.json\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Surplus Line (Dashboard 2)",
   "id": "727215811970eeeb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T12:57:38.498624Z",
     "start_time": "2025-04-05T12:57:30.202336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_surplus_trend(\n",
    "    solver_address,\n",
    "    from_token,\n",
    "    to_token,\n",
    "    view=\"7d\",  # or \"24h\"\n",
    "    competition_path=\"data/solver_competitions.csv\",\n",
    "    order_path=\"data/order.csv\",\n",
    "    token_labels_path=\"data/token_labels.csv\",\n",
    "    output_path_template=\"out/surplus_trend_{solver}_{fromtoken}_{totoken}_{view}.json\"\n",
    "):\n",
    "    ...\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(competition_path)\n",
    "    orders = pd.read_csv(order_path)\n",
    "    labels = pd.read_csv(token_labels_path, on_bad_lines=\"skip\")[[\"address\", \"symbol\"]]\n",
    "\n",
    "    # Filter by solver\n",
    "    df = df[df[\"solverAddress\"] == solver_address]\n",
    "\n",
    "    # Merge with order info for timestamp and token addresses\n",
    "    df = pd.merge(df, orders[[\"id\", \"createdTimestamp\", \"sellTokenAddress\", \"buyTokenAddress\"]],\n",
    "                  left_on=\"orderId\", right_on=\"id\", how=\"left\")\n",
    "    df[\"createdTimestamp\"] = pd.to_datetime(df[\"createdTimestamp\"], errors=\"coerce\")\n",
    "\n",
    "    # Filter time range\n",
    "    # now = pd.Timestamp.utcnow()\n",
    "    now = df[\"createdTimestamp\"].max()\n",
    "    if view == \"24h\":\n",
    "        df = df[df[\"createdTimestamp\"] >= now - timedelta(days=1)]\n",
    "    elif view == \"7d\":\n",
    "        df = df[df[\"createdTimestamp\"] >= now - timedelta(days=7)]\n",
    "    else:\n",
    "        raise ValueError(\"view must be '24h' or '7d'\")\n",
    "\n",
    "\n",
    "\n",
    "    # Map token addresses to symbols\n",
    "    df = pd.merge(df, labels, left_on=\"sellTokenAddress\", right_on=\"address\", how=\"left\").rename(columns={\"symbol\": \"sellSymbol\"})\n",
    "    df = pd.merge(df, labels, left_on=\"buyTokenAddress\", right_on=\"address\", how=\"left\").rename(columns={\"symbol\": \"buySymbol\"})\n",
    "\n",
    "    # Clean and filter\n",
    "    df = df.dropna(subset=[\"createdTimestamp\", \"surplus\", \"sellSymbol\", \"buySymbol\"])\n",
    "    df = df[(df[\"sellSymbol\"] == from_token) & (df[\"buySymbol\"] == to_token)]\n",
    "\n",
    "    # After final filtering\n",
    "    df = df[(df[\"sellSymbol\"] == from_token) & (df[\"buySymbol\"] == to_token)]\n",
    "\n",
    "    # Format output\n",
    "    df = df.sort_values(\"createdTimestamp\")\n",
    "    df[\"createdTimestamp\"] = df[\"createdTimestamp\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "    df[\"tokenPair\"] = df.apply(lambda row: f\"{row['sellSymbol']} â†’ {row['buySymbol']}\", axis=1)\n",
    "\n",
    "    result = df[[\"createdTimestamp\", \"sellSymbol\", \"buySymbol\", \"tokenPair\", \"surplus\"]]\n",
    "\n",
    "    # Write file\n",
    "    Path(\"out\").mkdir(parents=True, exist_ok=True)\n",
    "    from_token_safe = from_token.replace(\"/\", \"-\")\n",
    "    to_token_safe = to_token.replace(\"/\", \"-\")\n",
    "    out_path = output_path_template.format(\n",
    "        solver=solver_address[-6:], fromtoken=from_token.replace(\"/\", \"-\"), totoken=to_token.replace(\"/\", \"-\"), view=view\n",
    "    )\n",
    "\n",
    "    result.to_json(out_path, orient=\"records\", indent=2)\n",
    "    print(f\"[âœ“] Surplus trend written to {out_path} with {len(result)} entries.\")\n",
    "\n",
    "\n",
    "generate_surplus_trend(\n",
    "    solver_address=\"0x00806daa2cfe49715ea05243ff259deb195760fc\",\n",
    "    from_token=\"Wrapped Ether\",\n",
    "    to_token=\"USD Coin\"\n",
    ")\n",
    "\n",
    "\n"
   ],
   "id": "8654950ba29abac4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Surplus trend written to out/surplus_trend_5760fc_Wrapped Ether_USD Coin_7d.json with 489 entries.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Bubble Chart (Dashboard 2)",
   "id": "8d75057194fda131"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:08:28.832467Z",
     "start_time": "2025-04-05T13:08:14.737494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_solver_bubble_data(\n",
    "    solver_address,\n",
    "    token_labels_path=\"data/token_labels.csv\",\n",
    "    order_path=\"data/order.csv\",\n",
    "    competition_path=\"data/solver_competitions.csv\",\n",
    "    output_path=\"out/bubble_solver_volume_{solver}.json\"\n",
    "):\n",
    "    # Load input files\n",
    "    comp_df = pd.read_csv(competition_path)\n",
    "    order_df = pd.read_csv(order_path)\n",
    "    labels = pd.read_csv(token_labels_path, on_bad_lines=\"skip\")[[\"address\", \"symbol\"]]\n",
    "\n",
    "    order_df = order_df.rename(columns={\n",
    "        \"buyAmountInUSD\": \"orderBuyUSD\",\n",
    "        \"sellAmountInUSD\": \"orderSellUSD\",\n",
    "    })\n",
    "\n",
    "    # Filter by solver\n",
    "    comp_df = comp_df[comp_df[\"solverAddress\"] == solver_address]\n",
    "\n",
    "    # Merge orders with competition data\n",
    "    merged = pd.merge(\n",
    "        comp_df,\n",
    "        order_df,\n",
    "        left_on=\"orderId\",\n",
    "        right_on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Add token labels\n",
    "    merged = pd.merge(\n",
    "        merged, labels, left_on=\"sellTokenAddress\", right_on=\"address\", how=\"left\"\n",
    "    ).rename(columns={\"symbol\": \"sellSymbol\"})\n",
    "    merged = pd.merge(\n",
    "        merged, labels, left_on=\"buyTokenAddress\", right_on=\"address\", how=\"left\"\n",
    "    ).rename(columns={\"symbol\": \"buySymbol\"})\n",
    "\n",
    "    # Drop missing values\n",
    "    merged = merged.dropna(subset=[\"sellSymbol\", \"buySymbol\", \"buyAmountInUSD\", \"sellAmountInUSD\"])\n",
    "\n",
    "    # Create normalized token pair name\n",
    "    merged[\"tokenPair\"] = merged.apply(\n",
    "        lambda row: \"-\".join(sorted([row[\"sellSymbol\"], row[\"buySymbol\"]])),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Compute per-row volume\n",
    "    merged[\"volume\"] = merged[[\"buyAmountInUSD\", \"sellAmountInUSD\"]].min(axis=1)\n",
    "\n",
    "    # Bin definitions (same as treemap)\n",
    "    bins = [0, 1000] + [1000 + i * 1000 for i in range(1, 11)] + [np.inf]\n",
    "    labels_bins = [\n",
    "        f\"{int(bins[i])}-{int(bins[i + 1]) if bins[i + 1] != np.inf else '10000+'}\"\n",
    "        for i in range(len(bins) - 1)\n",
    "    ]\n",
    "    merged[\"volume_bin\"] = pd.cut(merged[\"volume\"], bins=bins, labels=labels_bins, right=False)\n",
    "\n",
    "    # Group by token pair\n",
    "    bubble_data = []\n",
    "    for token_pair, group in merged.groupby(\"tokenPair\"):\n",
    "        bin_counts = group[\"volume_bin\"].value_counts().reindex(labels_bins, fill_value=0).to_dict()\n",
    "        total_volume = group[\"volume\"].sum()\n",
    "\n",
    "        bubble_data.append({\n",
    "            \"tokenPair\": token_pair,\n",
    "            \"value\": total_volume,\n",
    "            \"binCounts\": bin_counts\n",
    "        })\n",
    "\n",
    "    # Keep top 50 by volume\n",
    "    bubble_data = sorted(bubble_data, key=lambda x: x[\"value\"], reverse=True)[:50]\n",
    "\n",
    "    # Save output\n",
    "    suffix = solver_address[-6:]\n",
    "    output_file = output_path.format(solver=suffix)\n",
    "    Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame(bubble_data).to_json(output_file, orient=\"records\", indent=2)\n",
    "\n",
    "    print(f\"[âœ“] Saved {len(bubble_data)} token pairs with volume + binCounts to {output_file}\")\n",
    "\n",
    "generate_solver_bubble_data(\"0x00806daa2cfe49715ea05243ff259deb195760fc\")"
   ],
   "id": "23e72f68ba8bb24f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Saved 50 token pairs with volume + binCounts to out/bubble_solver_volume_5760fc.json\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Solver Filled Donut Chart (Dashboard 2)",
   "id": "e0babee67ffc5b0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T14:17:46.040055Z",
     "start_time": "2025-04-05T14:17:38.618749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def generate_partially_fillable_donut_data(\n",
    "    solver_address,\n",
    "    order_path=\"data/order.csv\",\n",
    "    competition_path=\"data/solver_competitions.csv\",\n",
    "    output_path=\"out/partially_fillable_{solver}.json\"\n",
    "):\n",
    "    # Load input files\n",
    "    comp_df = pd.read_csv(competition_path)\n",
    "    order_df = pd.read_csv(order_path)\n",
    "\n",
    "    # Filter by solver\n",
    "    comp_df = comp_df[comp_df[\"solverAddress\"] == solver_address]\n",
    "\n",
    "    # Merge with order data to get partiallyFillable field\n",
    "    merged = pd.merge(\n",
    "        comp_df,\n",
    "        order_df[[\"id\", \"partiallyFillable\"]],\n",
    "        left_on=\"orderId\",\n",
    "        right_on=\"id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Convert to bool and count\n",
    "    merged[\"partiallyFillable\"] = merged[\"partiallyFillable\"].astype(bool)\n",
    "    counts = merged[\"partiallyFillable\"].value_counts().to_dict()\n",
    "\n",
    "    # Match format: key = label, value = count\n",
    "    binCounts = {\n",
    "        \"True\": counts.get(True, 0),\n",
    "        \"False\": counts.get(False, 0)\n",
    "    }\n",
    "\n",
    "    # Save output\n",
    "    suffix = solver_address[-6:]\n",
    "    output_file = output_path.format(solver=suffix)\n",
    "    Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump({\"binCounts\": binCounts}, f, indent=2)\n",
    "\n",
    "    print(f\"[âœ“] Saved partiallyFillable donut data for solver {solver_address} to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "generate_partially_fillable_donut_data(\"0x00806daa2cfe49715ea05243ff259deb195760fc\")\n"
   ],
   "id": "b7170c489698ca4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Saved partiallyFillable donut data for solver 0x00806daa2cfe49715ea05243ff259deb195760fc to out/partially_fillable_5760fc.json\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
